# Rock-Paper-Scissor_ImageGame

Final project for SCC0251 - Image Processing @ ICMC/USP.
* 10284890 - Ã“liver Savastano Becker (Image Filtering, DataSet creation, Unity Aplication)
* 10295412 - Rafael Farias Roque (Image Filtering, DataSet creation, Classifier)

# Files
* Demo.ipynb contains the usege a damonstraition of the working classification code
* Classifier.ipynb contains the creation and training of the classifier
* DataSet.ipynb contains the creation of a dataset based on the images in the 'fotos' folder
* ModelTest.ipynb contains a base line for the application script
* RockPaperScissor.ipynb contains a test of all ideas for the project

(The project was develop using Google CoLab and Google Drive and the commits do not represent the project colaboration)

# Abstract

The objective of this project is to create an app of Rock Paper Scissor game, played using images of human hands. The game will take a photo of the player's hand and identify the symbol made, then use this as input to get the game result.
There will be 2 playing modes: Vs CPU and Vs player.

To obtain the symbol made by the player, we are going to apply filters to enhance the image and then use it in a classifier.

# Report

## Objective
The objective of this project is to create an application that allows the user to play rock-paper-scissors game using pictures of it's own hands. The player will be able to take a picture of its hands using a phone and that picture will be processed and classified as one of the three possible choices: rock, when the hand is closed; paper, when the hand is wide open; and scissor, when only two fingers are raised. 

Initially we are going to implement only single player mode, which the player will play against the computer, however we are aiming to implement a player versus player mode in the future.

## Description of input images
We are going to use hand pictures with the three desired symbols in different angles to test the classificator. To get these images will be photographing our own hand and asking for familiars/friends to send some photos of theirs too. This way we can get some variety of hand types. There are three restrictions to the pictures: first the photo must contain the whole hand inside it, second the background must be clear (without much objects) and third the photographs must be taken using the flashlight to minimize shadows.

## Steps
After obtaining the images, it was necessary to perform a pre-processing so that we can extract the features that will be used for training the classifier. As the images may have different resolutions, the first step was to standardize / reduce the size of the images. It was defined that an image of size 150x150 has enough resolution to maintain the characteristics of the original image. In this case, colors are not relevant because we want to classify the shape of the object, thus we transformed the image to a grayscale. This is made by switching the color system from RGB to HSV and using only the Value channel, which has the intensities of each pixel.

After transforming the image to grayscale and resizing it, we apply an edge detection method. Initially, we thought that only these processing would be sufficient for the classification, thus we applied the filtered images in a MultiLayer Perceptron. However, the results obtained were not satisfactory and we decided to look for some features to try to better explain the content of the set of images.

The first feature chosen for classification was the LBP (Local Binary Pattern). From the matrices generated by the method, histograms with the frequencies of each of the present values were constructed. The second feature used was the HOG (Histogram of Oriented Gradients), which when applied together with the LBP presents very good results for the classification. With these new features, the classifier achieved a performance far superior to that obtained previously.

As our image set was relatively small, we augmented the data set by rotating and flipping the images. In addition, to facilitate reading, the images were transformed into an array, in which the last position represents their respective class, and saved in a CSV file.

The classifier used in the program was the MultiLayer Perceptron provided by the sklearn library and the method used to perform training and model validation was the 10-Fold Cross-Validation.


## Results

With the original pre-processing we achieved an accuracy that was around 17%, we tested several configurations of classifiers, but none had a significant improvement. After applying the LBP we had a small improvement, reaching an accuracy of 38%, but it was not yet an adequate value for the application.
With the HOG we had a great performance, reaching 73% accuracy. We decided to increase the data set, as we had a small data set. With the additions, the accuracy dropped to 72%, a minor drop as the data set increased six times.

About the application (the game implementation), we are stuck at the communication between the game engine and our python classifier. We are still trying to fix this in order to be able to finish the game.
